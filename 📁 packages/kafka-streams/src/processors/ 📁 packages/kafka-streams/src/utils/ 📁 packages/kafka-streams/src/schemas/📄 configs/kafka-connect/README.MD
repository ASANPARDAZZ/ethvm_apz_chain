ğŸš€ Ø³Ø§Ø®ØªØ§Ø± Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ EthVM Ø¨Ø±Ø§ÛŒ APZ Chain Ø¨Ø§ Kafka Streams

ğŸ“ Ø³Ø§Ø®ØªØ§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø±ÛŒÙ¾ÙˆØ²ÛŒØªÙˆØ±ÛŒ GitHub

```
ethvm-apz-chain/
â”œâ”€â”€ ğŸ“ .github/
â”‚   â””â”€â”€ ğŸ“ workflows/
â”‚       â”œâ”€â”€ ci-cd.yml
â”‚       â””â”€â”€ docker-publish.yml
â”œâ”€â”€ ğŸ“ configs/
â”‚   â”œâ”€â”€ kafka-connect/
â”‚   â”‚   â”œâ”€â”€ apz-source-connector.json
â”‚   â”‚   â””â”€â”€ postgres-sink-connector.json
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â””â”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ dashboard.yml
â”œâ”€â”€ ğŸ“ docker/
â”‚   â”œâ”€â”€ kafka-cluster.yml
â”‚   â”œâ”€â”€ monitoring.yml
â”‚   â””â”€â”€ services.yml
â”œâ”€â”€ ğŸ“ packages/
â”‚   â”œâ”€â”€ ğŸ“ kafka-streams/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ processors/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ apz-transaction-processor.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ apz-token-processor.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ apz-block-processor.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ schemas/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ transaction.avsc
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ block.avsc
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ token-transfer.avsc
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ kafka-config.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ metrics.ts
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ ğŸ“ api-service/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ models/
â”‚   â”‚   â”‚   â””â”€â”€ server.ts
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ ğŸ“ frontend/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ views/
â”‚   â”‚   â”‚   â””â”€â”€ main.ts
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ ğŸ“ common/
â”‚       â”œâ”€â”€ ğŸ“ types/
â”‚       â”œâ”€â”€ ğŸ“ utils/
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ ğŸ“ database/
â”‚   â”œâ”€â”€ migrations/
â”‚   â”œâ”€â”€ seeders/
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”œâ”€â”€ monitoring.sh
â”‚   â””â”€â”€ kafka-setup.sh
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ package.json
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

---

ğŸ¯ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Kafka Streams Ø¨Ø±Ø§ÛŒ ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§ÛŒ APZ

ğŸ“ packages/kafka-streams/src/processors/apz-transaction-processor.ts

```typescript
import { Kafka, KafkaJS, logLevel } from 'kafkajs';
import { Registry, AvroSerializer } from '@kafkajs/confluent-schema-registry';
import { PrometheusMetrics } from '../utils/metrics';
import { APZTransaction, APZBlock, APZTokenTransfer } from '../../../common/types';

export class APZTransactionProcessor {
  private kafka: Kafka;
  private registry: Registry;
  private metrics: PrometheusMetrics;
  private producer: KafkaJS.Producer;
  private consumer: KafkaJS.Consumer;

  constructor(
    private readonly config: {
      kafkaBrokers: string[];
      schemaRegistryUrl: string;
      sourceTopic: string;
      outputTopic: string;
      groupId: string;
    }
  ) {
    this.kafka = new Kafka({
      clientId: 'apz-transaction-processor',
      brokers: config.kafkaBrokers,
      logLevel: logLevel.INFO,
    });

    this.registry = new Registry({
      host: config.schemaRegistryUrl
    });

    this.metrics = new PrometheusMetrics();
    this.producer = this.kafka.producer();
    this.consumer = this.kafka.consumer({ groupId: config.groupId });
  }

  async start(): Promise<void> {
    await this.connect();
    await this.setupConsumer();

    console.log('ğŸš€ APZ Transaction Processor started successfully');
  }

  private async connect(): Promise<void> {
    await this.producer.connect();
    await this.consumer.connect();
    
    await this.consumer.subscribe({ 
      topic: this.config.sourceTopic, 
      fromBeginning: false 
    });
  }

  private async setupConsumer(): Promise<void> {
    await this.consumer.run({
      eachMessage: async ({ topic, partition, message }) => {
        try {
          const startTime = Date.now();
          
          // Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Avro
          const decodedMessage = await this.registry.decode(message.value);
          const processedData = await this.processTransaction(decodedMessage);

          // ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ§Ù… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
          await this.produceProcessedMessage(processedData);

          // Ø«Ø¨Øª Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
          this.metrics.recordTransactionProcessed(
            'success',
            Date.now() - startTime
          );

        } catch (error) {
          console.error('Error processing transaction:', error);
          this.metrics.recordTransactionProcessed('error', 0);
        }
      },
    });
  }

  private async processTransaction(rawTransaction: any): Promise<APZTransaction> {
    // Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ±Ø§Ú©Ù†Ø´ APZ
    const processedTx: APZTransaction = {
      hash: rawTransaction.hash,
      blockNumber: rawTransaction.blockNumber,
      blockHash: rawTransaction.blockHash,
      from: rawTransaction.from,
      to: rawTransaction.to,
      value: this.convertHexToWei(rawTransaction.value),
      gas: BigInt(rawTransaction.gas),
      gasPrice: BigInt(rawTransaction.gasPrice),
      input: rawTransaction.input,
      nonce: rawTransaction.nonce,
      transactionIndex: rawTransaction.transactionIndex,
      timestamp: Date.now(),
      status: 'pending',
      chainId: this.config.chainId || 'apz-mainnet',
      
      // ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ APZ Chain
      apzSpecific: {
        tokenTransfers: await this.extractTokenTransfers(rawTransaction),
        contractInteractions: this.parseContractInteractions(rawTransaction),
        fee: this.calculateTransactionFee(rawTransaction)
      }
    };

    // Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ØªØ±Ø§Ú©Ù†Ø´
    this.validateTransaction(processedTx);

    return processedTx;
  }

  private async extractTokenTransfers(tx: any): Promise<APZTokenTransfer[]> {
    const transfers: APZTokenTransfer[] = [];

    // Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†ØªÙ‚Ø§Ù„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ APZ
    if (tx.input && tx.input !== '0x') {
      try {
        // ØªØ´Ø®ÛŒØµ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ ØªÙˆÚ©Ù† (ERC-20, ERC-721, etc.)
        const tokenStandard = this.detectTokenStandard(tx.input);
        
        if (tokenStandard === 'ERC20') {
          const transfer = this.parseERC20Transfer(tx);
          if (transfer) transfers.push(transfer);
        }

        // Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ± Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§
        if (tokenStandard === 'ERC721') {
          const transfer = this.parseERC721Transfer(tx);
          if (transfer) transfers.push(transfer);
        }

      } catch (error) {
        console.warn('Failed to extract token transfers:', error);
      }
    }

    return transfers;
  }

  private parseERC20Transfer(tx: any): APZTokenTransfer | null {
    // Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§ÛŒ ERC-20
    try {
      // Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ ABI Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ APZ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆØ¯
      return {
        type: 'ERC20',
        from: tx.from,
        to: this.decodeAddress(tx.input, 4), // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø¯Ø±Ø³ Ù…Ù‚ØµØ¯ Ø§Ø² input
        value: this.decodeUint256(tx.input, 36), // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø¯Ø§Ø± Ø§Ø² input
        tokenAddress: tx.to,
        transactionHash: tx.hash,
        logIndex: 0
      };
    } catch (error) {
      return null;
    }
  }

  private calculateTransactionFee(tx: any): bigint {
    const gasUsed = BigInt(tx.gasUsed || tx.gas);
    const gasPrice = BigInt(tx.gasPrice);
    return gasUsed * gasPrice;
  }

  private validateTransaction(tx: APZTransaction): void {
    const errors: string[] = [];

    if (!tx.hash || tx.hash.length !== 66) {
      errors.push('Invalid transaction hash');
    }

    if (!tx.from || !this.isValidAddress(tx.from)) {
      errors.push('Invalid from address');
    }

    if (tx.to && !this.isValidAddress(tx.to)) {
      errors.push('Invalid to address');
    }

    if (errors.length > 0) {
      throw new Error(`Transaction validation failed: ${errors.join(', ')}`);
    }
  }

  private isValidAddress(address: string): boolean {
    return /^0x[a-fA-F0-9]{40}$/.test(address);
  }

  private convertHexToWei(hexValue: string): bigint {
    return BigInt(hexValue);
  }

  private async produceProcessedMessage(processedTx: APZTransaction): Promise<void> {
    const encodedMessage = await this.registry.encode(processedTx);
    
    await this.producer.send({
      topic: this.config.outputTopic,
      messages: [
        {
          key: processedTx.hash,
          value: encodedMessage
        }
      ]
    });
  }

  async shutdown(): Promise<void> {
    await this.consumer.disconnect();
    await this.producer.disconnect();
    console.log('APZ Transaction Processor shutdown complete');
  }
}
```

---

ğŸ“ packages/kafka-streams/src/processors/apz-block-processor.ts

```typescript
import { Kafka } from 'kafkajs';

export class APZBlockProcessor {
  constructor(private kafka: Kafka) {}

  async processBlock(rawBlock: any): Promise<any> {
    const processedBlock = {
      ...rawBlock,
      apzMetrics: {
        totalDifficulty: BigInt(rawBlock.totalDifficulty),
        size: rawBlock.size,
        gasUsed: BigInt(rawBlock.gasUsed),
        gasLimit: BigInt(rawBlock.gasLimit),
        timestamp: new Date(parseInt(rawBlock.timestamp) * 1000),
        transactionCount: rawBlock.transactions.length,
        uncleCount: rawBlock.uncles.length
      },
      processedAt: new Date().toISOString()
    };

    // Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø²Ù†Ø¬ÛŒØ±Ù‡ APZ
    const chainMetrics = this.calculateChainMetrics(processedBlock);

    return {
      ...processedBlock,
      chainMetrics
    };
  }

  private calculateChainMetrics(block: any) {
    return {
      tps: this.calculateTPS(block),
      blockTime: this.calculateBlockTime(block),
      gasUtilization: Number(block.apzMetrics.gasUsed) / Number(block.apzMetrics.gasLimit),
      difficulty: Number(block.apzMetrics.totalDifficulty)
    };
  }

  private calculateTPS(block: any): number {
    // Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ±Ø§Ú©Ù†Ø´ Ø¯Ø± Ø«Ø§Ù†ÛŒÙ‡
    return block.apzMetrics.transactionCount / 15; // ÙØ±Ø¶ Ø¨Ø± 15 Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ù„Ø§Ú© ØªØ§ÛŒÙ…
  }

  private calculateBlockTime(block: any): number {
    // Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† Ø¨ÛŒÙ† Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§
    return 15; // Ù…Ù‚Ø¯Ø§Ø± Ù†Ù…ÙˆÙ†Ù‡
  }
}
```

---

ğŸ“ packages/kafka-streams/src/utils/metrics.ts

```typescript
import client from 'prom-client';

export class PrometheusMetrics {
  private transactionCounter: client.Counter;
  private processingDuration: client.Histogram;
  private errorCounter: client.Counter;
  private blockGauge: client.Gauge;

  constructor() {
    this.transactionCounter = new client.Counter({
      name: 'apz_transactions_processed_total',
      help: 'Total number of APZ transactions processed',
      labelNames: ['status', 'type']
    });

    this.processingDuration = new client.Histogram({
      name: 'apz_processing_duration_seconds',
      help: 'Time taken to process APZ transactions',
      buckets: [0.1, 0.5, 1, 2, 5]
    });

    this.errorCounter = new client.Counter({
      name: 'apz_processing_errors_total',
      help: 'Total number of processing errors',
      labelNames: ['error_type']
    });

    this.blockGauge = new client.Gauge({
      name: 'apz_latest_block_number',
      help: 'Latest APZ block number processed'
    });
  }

  recordTransactionProcessed(status: string, duration: number): void {
    this.transactionCounter.inc({ status });
    this.processingDuration.observe(duration / 1000);
  }

  recordError(errorType: string): void {
    this.errorCounter.inc({ error_type: errorType });
  }

  updateLatestBlock(blockNumber: number): void {
    this.blockGauge.set(blockNumber);
  }
}
```

---

ğŸ“ packages/kafka-streams/src/schemas/transaction.avsc

```json
{
  "type": "record",
  "name": "APZTransaction",
  "namespace": "com.ethvm.apz",
  "fields": [
    {
      "name": "hash",
      "type": "string"
    },
    {
      "name": "blockNumber",
      "type": "long"
    },
    {
      "name": "from",
      "type": "string"
    },
    {
      "name": "to",
      "type": ["null", "string"]
    },
    {
      "name": "value",
      "type": "string"
    },
    {
      "name": "gas",
      "type": "long"
    },
    {
      "name": "gasPrice",
      "type": "long"
    },
    {
      "name": "input",
      "type": "string"
    },
    {
      "name": "timestamp",
      "type": "long"
    },
    {
      "name": "status",
      "type": {
        "type": "enum",
        "name": "TransactionStatus",
        "symbols": ["pending", "confirmed", "failed"]
      }
    },
    {
      "name": "apzSpecific",
      "type": {
        "type": "record",
        "name": "APZSpecificData",
        "fields": [
          {
            "name": "tokenTransfers",
            "type": {
              "type": "array",
              "items": {
                "type": "record",
                "name": "TokenTransfer",
                "fields": [
                  {"name": "type", "type": "string"},
                  {"name": "from", "type": "string"},
                  {"name": "to", "type": "string"},
                  {"name": "value", "type": "string"},
                  {"name": "tokenAddress", "type": "string"}
                ]
              }
            }
          },
          {
            "name": "fee",
            "type": "long"
          }
        ]
      }
    }
  ]
}
```

---

ğŸ³ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Docker

ğŸ“„ docker-compose.yml

```yaml
version: '3.8'

services:
  # Ú©Ø§ÙÚ©Ø§ Ú©Ù„Ø§Ø³ØªØ±
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_NUM_PARTITIONS: 3
    ports:
      - "9092:9092"

  schema-registry:
    image: confluentinc/cp-schema-registry:7.3.0
    depends_on:
      - kafka
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    ports:
      - "8081:8081"

  # Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±Ù‡Ø§ÛŒ APZ
  apz-transaction-processor:
    build:
      context: ./packages/kafka-streams
      dockerfile: Dockerfile
    environment:
      KAFKA_BROKERS: kafka:9092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      SOURCE_TOPIC: raw-apz-blocks
      OUTPUT_TOPIC: processed-apz-transactions
      GROUP_ID: apz-transaction-processor
      METRICS_PORT: 9090
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "9095:9090"

  # Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./configs/grafana/dashboard.yml:/etc/grafana/provisioning/dashboards/

  # Ø¯ÛŒØªØ§Ø¨ÛŒØ³
  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: apz_indexer
      POSTGRES_USER: apz_user
      POSTGRES_PASSWORD: apz_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
  kafka_data:
```

---

ğŸ”§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ

ğŸ“„ configs/kafka-connect/apz-source-connector.json

```json
{
  "name": "apz-blockchain-source",
  "config": {
    "connector.class": "io.confluent.connect.ethereum.EthereumBlockConnector",
    "tasks.max": "1",
    "ethereum.url": "https://rpc.apzchain.org",
    "topic": "raw-apz-blocks",
    "start.block": "latest",
    "batch.size": "100",
    "schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081"
  }
}
```

---

ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§

ğŸ“„ README.md

```markdown
# EthVM for APZ Chain with Kafka Streams

ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø² EthVM Ø¨Ø±Ø§ÛŒ APZ Chain Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±Ù‡Ø§ÛŒ Kafka Streams.

## ğŸš€ Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹

### Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§
- Docker & Docker Compose
- Node.js 18+
- pnpm

### Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ

1. **Ú©Ù„ÙˆÙ† Ú©Ø±Ø¯Ù† Ø±ÛŒÙ¾ÙˆØ²ÛŒØªÙˆØ±ÛŒ**
```bash
git clone https://github.com/your-username/ethvm-apz-chain.git
cd ethvm-apz-chain
```

1. Ú©Ù¾ÛŒ ÙØ§ÛŒÙ„ Ù…Ø­ÛŒØ·ÛŒ

```bash
cp .env.example .env
```

1. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§

```bash
docker-compose up -d
```

1. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±Ù‡Ø§

```bash
pnpm install
pnpm build
pnpm start:processors
```

ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯

Â· Grafana: http://localhost:3000 (admin/admin)
Â· Prometheus: http://localhost:9090
Â· Kafka UI: http://localhost:8080

ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ

Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø§Ø² Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

1. Data Ingestion: Kafka Connect Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² APZ RPC
2. Stream Processing: Kafka Streams Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§
3. Storage: PostgreSQL Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
4. API: NestJS Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ API
5. Frontend: Vue.js Ø¨Ø±Ø§ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ

```

---

### ğŸ“„ `package.json` Ø§ØµÙ„ÛŒ

```json
{
  "name": "ethvm-apz-chain",
  "version": "1.0.0",
  "description": "EthVM implementation for APZ Chain with Kafka Streams",
  "scripts": {
    "dev": "concurrently \"pnpm dev:processors\" \"pnpm dev:api\" \"pnpm dev:frontend\"",
    "dev:processors": "cd packages/kafka-streams && pnpm dev",
    "build": "lerna run build",
    "test": "lerna run test",
    "docker:build": "docker-compose build",
    "docker:up": "docker-compose up -d",
    "docker:down": "docker-compose down",
    "deploy:prod": "./scripts/deploy.sh production"
  },
  "devDependencies": {
    "lerna": "^6.0.0",
    "concurrently": "^7.0.0",
    "typescript": "^4.9.0"
  }
}
```

---

ğŸ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¯Ø± GitHub

1. Ø§ÛŒØ¬Ø§Ø¯ Ø±ÛŒÙ¾ÙˆØ²ÛŒØªÙˆØ±ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± GitHub
2. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:

```bash
git init
git add .
git commit -m "feat: initial EthVM APZ Chain implementation with Kafka Streams"
git branch -M main
git remote add origin https://github.com/your-username/ethvm-apz-chain.git
git push -u origin main
```

1. ØªÙ†Ø¸ÛŒÙ… GitHub Actions (Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² .github/workflows/ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯)

Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ú©Ø§Ù…Ù„ØŒ ØªÙ…Ø§Ù… Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ EthVM Ø¨Ø±Ø§ÛŒ APZ Chain Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±Ù‡Ø§ÛŒ Kafka Streams Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
